# -*- coding: utf-8 -*-
"""2018A7PS0068U - Code - FDS Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a6A1RY43ZLAh1jEdUq9c_6928e5TOj1b
"""

#1.IMPORTING THE REQUIRED LIBRARIES AND DATASET AS .CSV file

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('classic')
# %matplotlib inline

df = pd.read_csv("/content/Melbourne_housing_FULL.csv")
df.head(5)

#2.CLEANING THE DATA - Checking for any null values present in the dataset and ploting the Outliers for further implementation.

df.isnull().sum()

df.plot(kind='box', subplots = True, layout = (4,4), fontsize = 10, figsize = (17,12));

#3. TRAINING THE DATA

df.columns

train_col = ['Suburb', 'Rooms', 'Method', 'Date', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',
             'YearBuilt', 'Lattitude', 'Longtitude', 'Distance', 'Price']
df_train = df[train_col].dropna().reset_index(drop=True)
df_train['Date'] = pd.to_datetime(df_train['Date'])

df_train['year'] = [x.year for x in df_train['Date']]
df_train['month'] = [x.month for x in df_train['Date']]

df_train.head(5)

#4. DATAFRAMES FOR REPLACEMENT VALUES for the required set.

suburban = list(df_train.Suburb.value_counts().index)
col_rep = train_col
for r in ['Method', 'Date', 'Suburb']:
    col_rep.remove(r) 
stat_col = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
end_col = [1, 1, 1, 1, 1, 0, 0, 0, 1, 1]

def search_vals(dx, x, z):
    dx_mean = dx.mean()
    dx_mode = dx.mode()[0]
    
    q1 = dx.quantile(0.25)
    q3 = dx.quantile(0.75)
    iqr = q3 - q1
    
    upper = q3 + 1.5 * iqr
    lower = q1 - 1.5 * iqr
    
    if x == 0:
        y = dx_mean
    else:
        y = dx_mode
        
    if (z == 1) & (lower < 0):
        lower = 0
        
    if z == 1:
        y = int(y)
        upper = int(upper)
        lower = int(lower)
    
    return round(y, 2), upper, lower

replacement = {}
for x in suburban:
    app = {}
    for c, i, j in zip(col_rep, stat_col, end_col):
        app[c] = list(search_vals(df_train[df_train['Suburb'] == x][c], i, j))
    replacement[x] = app

for i in range(4):
    print(suburban[i], '\n', replacement[suburban[i]], '\n')

#5. EDITING THE OUTLIERS

dx_suburb = df_train['Suburb']
for c in col_rep:
    change = []
    for i, s in enumerate(dx_suburb):
        v = df_train.loc[i, c]
        u = replacement[s][c][1]
        l = replacement[s][c][2]
        if v > u:
            v = u
        elif v < l:
            v = l
        change.append(v)
    df_train[c] = change

df_train.head(5)

#6. TRAINING THE MODEL to check for the error variations using MSE and MAPE

from sklearn.preprocessing import MinMaxScaler

x_cols = ['Rooms', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt',
          'Lattitude', 'Longtitude', 'Distance', 'year', 'month']
train_x = df_train[x_cols]
train_y = df_train['Price']

scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

new_train_x = scaler_x.fit_transform(train_x)
new_train_y = scaler_y.fit_transform(np.array(train_y).reshape(-1, 1))

new_train_x

new_train_y

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error

x_train, x_valid, y_train, y_valid = train_test_split(new_train_x, new_train_y, test_size=0.2)

rf_model = RandomForestRegressor()
rf_model.fit(x_train, y_train)

y_pred = rf_model.predict(x_valid) 
error = np.sqrt(mean_squared_error(y_pred, y_valid))
error

error1= mean_absolute_percentage_error(y_valid, y_pred)
error1*100

#7. EDITING ALL OUTLIERS

da_suburb = df['Suburb'].value_counts().index
not_in_the_list = []
for d in da_suburb:
    if d not in suburban:
        not_in_the_list.append(d)
print('The suburb that is not in the list :')
for i, s in enumerate(not_in_the_list):
    print((i+1), s)

len(df[df.Suburb.isin(not_in_the_list)])

drop_ind = df[df.Suburb.isin(not_in_the_list)].index
df = df.drop(drop_ind)
df = df.reset_index(drop=True)
df = df.drop(['Bedroom2'], axis=1)

dx_suburb = df['Suburb']
for c in col_rep:
    changed = []
    for i, s in enumerate(dx_suburb):
        v = df.loc[i, c]
        u = replacement[s][c][1]
        l = replacement[s][c][2]
        if v > u:
            v = u
        elif v < l:
            v = l
        changed.append(v)
    df[c] = changed

#8. FILLING IN NULL VALUES

dfs = df['Suburb']
col_rep.remove('Price')

for c in col_rep:
    dfc = df[c]
    null = dfc.isnull()
    changed = []
    for i, d, s in zip(null, dfc, dfs):
        if  i == True:
            d = replacement[s][c][0] 
        changed.append(d)
    df[c] = changed

df.isnull().sum()

df['Date'] = pd.to_datetime(df['Date'])
df['year'] = [x.year for x in df['Date']]
df['month'] = [x.month for x in df['Date']]

from tqdm.auto import tqdm

price_null = df['Price'].isnull()
x_ = scaler_x.transform(df[x_cols])
new_ = []
progress_bar = tqdm(range(len(df)))

for n, i, p in zip(price_null, x_, df['Price']):
    if n == True:
        p = rf_model.predict(i.reshape(1, -1))
        p = scaler_y.inverse_transform(p.reshape(-1, 1))[0, 0]
    new_.append(p)
    progress_bar.update(1)
    
df['Price'] = new_

df.isnull().sum()

error1= mean_absolute_percentage_error(y_valid, y_pred)
error1*100

df = df.dropna()
df.head(5)

#9.ANAYLSING THE TRAINED DATA

arr = []
for i in df['Type']:
    if i == 'h':
        arr.append('House')
    elif i == 't':
        arr.append('Townhouse')
    elif i == 'u':
        arr.append('Duplex')
df['Type'] = arr

data = df[["Type", "Date", "Price"]]
d = data.groupby( by = ["Type", "Date"] ).mean()

new_type = []
new_date = []
new_price = []

for i, j in zip(d.index, d.Price):
    y, z = i
    new_type.append(y)
    new_date.append(z)
    new_price.append(j)
    
new_da = pd.DataFrame({"type":new_type, "date":new_date, "price":new_price})

plt.figure(figsize=(13, 8))
sns.lineplot(data=new_da, x="date", y="price", hue="type")
plt.tight_layout()

dx = df[["Regionname", "Suburb", "Price", "Lattitude", "Longtitude"]].groupby(by=["Regionname", "Suburb"]).mean()
dx_region = [i for i, j in dx.index]
dx_suburb = [j for i, j in dx.index]

dx["Region"] = dx_region
dx["Suburb"] = dx_suburb
dx = dx.reset_index(drop=True)

fig, ax = plt.subplots(2, 1, figsize=(12, 16))
sns.scatterplot(data=df, x="Lattitude", y="Longtitude", hue="Type", size="Rooms", ax=ax[0])
ax[0].set_title("House Locations")
sns.scatterplot(data=dx, x="Lattitude", y="Longtitude", hue="Region", ax=ax[1])
ax[1].set_title("Suburban Locations")
plt.tight_layout()

df.Regionname.value_counts()

d = df[["Regionname", "Date", "Price"]].groupby(by=["Regionname", "Date"]).mean()
region_d = []
date_d = []
price_d = []

for i, j in zip(d.index, d["Price"]):
    y, z = i
    region_d.append(y)
    date_d.append(z)
    price_d.append(j)
    
new_data = pd.DataFrame({"date":date_d, "region":region_d, "price":price_d})

metro_area = ["Southern Metropolitan", "Northern Metropolitan", "Western Metropolitan",
                "Eastern Metropolitan", "South-Eastern Metropolitan"]
victoria_area = ["Eastern Victoria", "Northern Victoria", "Western Victoria"]

fig, ax = plt.subplots(2, 1, figsize=(11, 15))
sns.lineplot(data=new_data[new_data.region.isin(metro_area)], x="date", y="price", hue="region", ax=ax[0])
ax[0].set_title("Metropolitan Area's Price Fluctuation")
sns.lineplot(data=new_data[new_data.region.isin(victoria)], x="date", y="price", hue="region", ax=ax[1])
ax[1].set_title("Victoria Area's Price Fluctuation")

#End